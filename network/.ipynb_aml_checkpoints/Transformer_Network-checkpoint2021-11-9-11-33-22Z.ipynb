{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import einops\r\n",
        "from einops import rearrange"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1639049445010
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelEmbedding(nn.Module):\r\n",
        "    def __init__(self, N, embed_dim = 64):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        num_pixels = N*N\r\n",
        "\r\n",
        "        self.projection = nn.Linear(2, embed_dim)\r\n",
        "        self.positional_embeddings = nn.Parameter(torch.randn(N*N,embed_dim), requires_grad=True)\r\n",
        "\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        # Rearrange input pixel condition (Batch, 2, N, N) to (Batch, N*N, 2)\r\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\r\n",
        "\r\n",
        "        x = self.projection(x)\r\n",
        "        \r\n",
        "        # add position embedding\r\n",
        "        x += self.positional_embeddings\r\n",
        "\r\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639049445831
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, emb_dim = 64, d_model = 64, num_heads = 8, dropout = 0.1):\r\n",
        "        super().__init__()\r\n",
        "        self.emb_dim = emb_dim\r\n",
        "        self.num_heads = num_heads\r\n",
        "        self.d_model = d_model\r\n",
        "\r\n",
        "\r\n",
        "        self.key_map = nn.Linear(emb_dim, d_model)\r\n",
        "        self.query_map = nn.Linear(emb_dim, d_model)\r\n",
        "        self.value_map = nn.Linear(emb_dim, d_model)\r\n",
        "\r\n",
        "        self.d_k = d_model // num_heads\r\n",
        "\r\n",
        "        self.att_drop = nn.Dropout(dropout)\r\n",
        "        self.projection = nn.Linear(d_model, emb_dim)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        # No masking\r\n",
        "        # split keys, queries and values in num_heads\r\n",
        "\r\n",
        "        # n corresponds to N\r\n",
        "        # h d = d_model, h: number of heads, d: d_k\r\n",
        "\r\n",
        "        key = rearrange(self.key_map(x), 'b n (h d) -> b h n d', h = self.num_heads)\r\n",
        "        query = rearrange(self.query_map(x), \"b n (h d) -> b h n d\", h=self.num_heads)\r\n",
        "        value  = rearrange(self.value_map(x), \"b n (h d) -> b h n d\", h=self.num_heads)\r\n",
        "\r\n",
        "        \r\n",
        "        # sum up over the last axis\r\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', query, key) # batch, num_heads, query_len, key_len, same as transposing the last two dimension\r\n",
        "            \r\n",
        "        scaling = self.d_k ** (1/2)\r\n",
        "        att = F.softmax(energy, dim=-1) / scaling\r\n",
        "        att = self.att_drop(att)\r\n",
        "\r\n",
        "        # sum up over the third axis\r\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, value) # simple matrix multiplcation over the last dimension\r\n",
        "\r\n",
        "        # concatenate the head\r\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\r\n",
        "\r\n",
        "        out = self.projection(out)\r\n",
        "\r\n",
        "        return out"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639049446301
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Sequential):\r\n",
        "    def __init__(self, emb_dim = 64, expansion = 4, dropout = 0.1):\r\n",
        "        super().__init__(\r\n",
        "            nn.Linear(emb_dim, expansion * emb_dim),\r\n",
        "            nn.GELU(),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(expansion * emb_dim, emb_dim),\r\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639049446756
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder_Block(nn.Module):\r\n",
        "    def __init__(self, emb_dim = 64, d_model = 64, num_heads = 8, expansion = 4, dropout = 0.1):\r\n",
        "        super().__init__()\r\n",
        "        self.layernorm = nn.LayerNorm(emb_dim)\r\n",
        "        self.multiheadattention = MultiHeadAttention(emb_dim = emb_dim, d_model= d_model, num_heads = num_heads, dropout = dropout)\r\n",
        "        self.feedforward = FeedForwardBlock(emb_dim = emb_dim, expansion = expansion, dropout = dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        x = self.layernorm(x)\r\n",
        "        x = self.multiheadattention(x)\r\n",
        "        x = self.dropout(x)\r\n",
        "        x += identity\r\n",
        "\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        x = self.layernorm(x)\r\n",
        "        x = self.feedforward(x)\r\n",
        "        x = self.dropout(x)\r\n",
        "        x+= identity\r\n",
        "\r\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639049446972
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Sequential):\r\n",
        "    def __init__(self, depth = 4, **kwargs):\r\n",
        "        super().__init__(*[Transformer_Encoder_Block(**kwargs) for _ in range(depth)])"
      ],
      "outputs": [],
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639049447473
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ATnet(nn.Module):\r\n",
        "    def __init__(self, N = 10, emb_dim = 64, depth = 4, d_model = 64, num_heads = 8, expansion = 4, dropout = 0.1):\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.pixelembedding = PixelEmbedding(N, emb_dim)\r\n",
        "        self.transformerencoder = TransformerEncoder(depth = depth, emb_dim=emb_dim, d_model = d_model, \\\r\n",
        "        num_heads = num_heads, expansion = expansion, dropout = dropout)\r\n",
        "        self.conv = nn.Conv1d(emb_dim, 1, 1)\r\n",
        "        self.output = nn.Sigmoid()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        x = self.pixelembedding(x)\r\n",
        "        x = self.transformerencoder(x)\r\n",
        "        # (Batch, N*N, C) --> (Batch, C, N*N)\r\n",
        "        x = rearrange('b l c -> b c l')\r\n",
        "        x = self.conv(x)\r\n",
        "        x = torch.squeeze(x)\r\n",
        "        x = self.output(x)\r\n",
        "\r\n",
        "        return x\r\n"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1639049447948
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}